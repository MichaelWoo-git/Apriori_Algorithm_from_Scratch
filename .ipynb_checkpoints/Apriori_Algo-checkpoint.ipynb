{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "from itertools import permutations, combinations\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Prompts to choose which store you want "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to Apriori 2.0!\n"
     ]
    }
   ],
   "source": [
    "print(\"Welcome to Apriori 2.0!\")\n",
    "store_num = input(\"Please select your store \\n 1. Amazon \\n 2. Nike \\n 3. Best Buy \\n 4. K-Mart \\n 5. Walmart\\n\")\n",
    "print(store_num)\n",
    "support_percent = input(\"Please enter the percentage of Support you want?\\n\")\n",
    "print(support_percent)\n",
    "confidence_percent = input(\"Please enter the percentage of Confidence you want?\\n\")\n",
    "print(confidence_percent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### These are my dictionaries to choose which store to get based in Key-Value Pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def number_to_store(store_number):\n",
    "    switcher = {\n",
    "        1: \"data/amazon_transactions.csv\",\n",
    "        2: \"data/nike_transaction.csv\",\n",
    "        3: \"data/best_buy_transaction.csv\",\n",
    "        4: \"data/k_mart_transaction.csv\",\n",
    "        5: \"data/walmart_transaction.csv\"\n",
    "    }\n",
    "    return switcher.get(store_number)\n",
    "\n",
    "\n",
    "def number_to_item_list_of_store(store_number):\n",
    "    switcher_dict = {\n",
    "        1: \"data/amazon_item_names.csv\",\n",
    "        2: \"data/nike_item_names.csv\",\n",
    "        3: \"data/best_buy_item_names.csv\",\n",
    "        4: \"data/k_mart_item_names.csv\",\n",
    "        5: \"data/walmart_item_names.csv\"\n",
    "    }\n",
    "    return switcher_dict.get(store_number)\n",
    "\n",
    "def ns(store_number):\n",
    "    switcher_store = {\n",
    "        1: \"Amazon\",\n",
    "        2: \"Nike\",\n",
    "        3: \"Best Buy\",\n",
    "        4: \"K-Mart\",\n",
    "        5: \"Walmart\"\n",
    "    }\n",
    "    return switcher_store.get(store_number)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### We first have to read in the csv files and make sure that the inputs received from the user are valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def a_priori_read(item_list, transaction, support_percentage, confidence_percentage):\n",
    "    # Create two different functions one that is solo for read in the file data and the other that is algorithmic with the data\n",
    "    if support_percentage > 100 or confidence_percentage > 100 or support_percentage < 0 or confidence_percentage < 0:\n",
    "        print(\"Support Percent or Confidence Percent is Invalid. \\n Enter a valid number between 0 and 100.\\n\")\n",
    "        print(\"Restarting Apriori 2.0.....\\n\")\n",
    "        time.sleep(2)\n",
    "        os.system(\"python Aprior_Algo\")\n",
    "    if support_percentage >= 0 and support_percentage <= 100 and confidence_percentage >= 0 and confidence_percentage <= 100:\n",
    "        df_item_list = pd.read_csv(item_list)\n",
    "        df_transactions = pd.read_csv(transaction)\n",
    "        print(df_transactions.head())\n",
    "        print(df_item_list.head())\n",
    "        trans = np.array(df_transactions[\"transaction\"])\n",
    "        items_names = np.array(df_item_list[\"item_name\"])\n",
    "        k_value = 1\n",
    "        return items_names, trans, support_percentage, confidence_percentage, k_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The first go around of the Apriori Algorithm we find the items that are most frequent when K=1\n",
    "##### This is so that we can find the most frequent items given the transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_1(items_names, trans, support_percentage, confidence_percentage, k_value):\n",
    "    counter = np.zeros(len(items_names), dtype=int)\n",
    "    for i in trans:\n",
    "        i = list((map(str.strip, i.split(','))))\n",
    "        s1 = set(i)\n",
    "        nums = 0\n",
    "        for x in items_names:\n",
    "            s2 = set()\n",
    "            s2.add(x)\n",
    "            if s2.issubset(s1):\n",
    "                counter[nums] += 1\n",
    "            nums += 1\n",
    "    counter = list(map(lambda x: int((x / len(trans)) * 100), counter))\n",
    "    df3 = pd.DataFrame({\"item_name\": items_names, \"support\": counter,\"k_val\" : np.full(len(items_names),k_value)})\n",
    "    rslt_df = df3[df3['support'] >= support_percentage]\n",
    "    print(\"When K = \" + str(k_value))\n",
    "    print(rslt_df)\n",
    "    items = np.array(rslt_df[\"item_name\"])\n",
    "    support_count = np.array(rslt_df[\"support\"])\n",
    "    k_value += 1\n",
    "    return items, support_count, k_value, rslt_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Then we use this function below to find item sets that are most frequent when K > 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ap_2(item_comb, k_value, trans, support_percentage):\n",
    "    boo = True\n",
    "    comb = combinations(item_comb, k_value)\n",
    "    comb = list(comb)\n",
    "    counter = np.zeros(len(comb), dtype=int)\n",
    "    if k_value > 1:\n",
    "        for i in trans:\n",
    "            i = list((map(str.strip, i.split(','))))\n",
    "            s1 = set(i)\n",
    "            nums = 0\n",
    "            for x in comb:\n",
    "                s2 = set()\n",
    "                x = np.asarray(x)\n",
    "                for q in x:\n",
    "                    s2.add(q)\n",
    "                if s2.issubset(s1):\n",
    "                    counter[nums] += 1\n",
    "                nums += 1\n",
    "    counter = list(map(lambda x: int((x / len(trans)) * 100), counter))\n",
    "    df3 = pd.DataFrame({\"item_name\": comb, \"support\": counter,\"k_val\":np.full(len(comb),k_value)})\n",
    "    \n",
    "    #Making sure that user parameters are met for support\n",
    "    rslt_df = df3[df3['support'] >= support_percentage]\n",
    "    print(\"When K = \" + str(k_value))\n",
    "    print(rslt_df)\n",
    "    items = np.array(rslt_df[\"item_name\"])\n",
    "    supp = np.array(rslt_df[\"support\"])\n",
    "    if len(items) == 0:\n",
    "        boo = False\n",
    "        return rslt_df, boo\n",
    "    return rslt_df, boo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Calls of functions and variable saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "items_names, trans, support_percent, confidence_percent, k_value = a_priori_read(\n",
    "    str(number_to_item_list_of_store(int(store_num))), str(number_to_store(int(store_num))),\n",
    "    int(support_percent), int(confidence_percent))\n",
    "\n",
    "items, supp, k_value, df = ap_1(items_names, trans, support_percent, confidence_percent, k_value)\n",
    "frames.append(df)\n",
    "boo = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Increasing K by 1 until we can longer support the support value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while boo:\n",
    "    df_1, boo = ap_2(items, k_value, trans, support_percent)\n",
    "    frames.append(df_1)\n",
    "    k_value += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Combine the dataframes we have from when we increase K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"results of item-sets that meet support are below\")\n",
    "display(pd.concat(frames))\n",
    "df_supp = pd.concat(frames)\n",
    "# df_supp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reset the index just to organize it and the results after we find the most frequent sets in the list of transactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_supp = df_supp.reset_index().drop('index',axis=1)\n",
    "df_supp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### This is the FUNCTION that genrerates the Associations (Permutations) and calculating the Confidence of the item sets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def confidence(val):\n",
    "    \n",
    "    #first\n",
    "    df_212 = df_supp.loc[df_supp['k_val'] == val] \n",
    "    stuff_name = np.array(df_212[\"item_name\"])\n",
    "    support_arr = np.array(df_212['support'])\n",
    "    \n",
    "    #second\n",
    "    df_temp = df_supp.loc[df_supp['k_val'] == val+1] \n",
    "    df_21231 = np.array(df_temp[\"item_name\"])\n",
    "    sup_arr = np.array(df_temp['support'])\n",
    "    \n",
    "    #variables to save\n",
    "    sup_ov = list()\n",
    "    sup_sing = list()\n",
    "    perm_item = list()\n",
    "    \n",
    "    #When the item set is k =1 and the comparison is k = 2\n",
    "    if val == 1:\n",
    "        for w in df_21231:\n",
    "            temp_list = list(df_21231)\n",
    "            ov_sup = sup_arr[temp_list.index(w)]\n",
    "            temp = set()\n",
    "            for d in w:\n",
    "                temp.add(d)\n",
    "            perm = permutations(temp)\n",
    "            perm_lst = list(perm)\n",
    "            for y in perm_lst:\n",
    "                perm_item.append(y)\n",
    "                sup_ov.append(ov_sup)\n",
    "                sup_sing.append(int(support_arr[np.where(stuff_name == y[0])]))\n",
    "    \n",
    "    #When the item set is k > 1 and the comparison is k += k + 1\n",
    "    if val > 1:    \n",
    "        for w in df_21231:\n",
    "            temp_list = list(df_21231)\n",
    "            ov_sup = sup_arr[temp_list.index(w)]\n",
    "            temp = set()\n",
    "            for d in w:\n",
    "                temp.add(d)\n",
    "            perm = permutations(temp)\n",
    "            perm_lst = list(perm)\n",
    "            for y in perm_lst:\n",
    "                try:\n",
    "                    temp_set = []\n",
    "                    for t in range(0,val):\n",
    "                        temp_set.append(y[t])\n",
    "                    #lol = tuple(sorted(temp_set))\n",
    "                    lol = tuple(temp_set)\n",
    "                    tp_lst = list(stuff_name)\n",
    "                    ss = support_arr[tp_lst.index(lol)]\n",
    "                    sup_ov.append(ov_sup)\n",
    "                    sup_sing.append(ss)\n",
    "                    perm_item.append(y)\n",
    "                except:\n",
    "                    print(\"itemset below does not exist...\")\n",
    "                    print(y)\n",
    "                    sup_ov.append(ov_sup)\n",
    "                    sup_sing.append(0)\n",
    "                    perm_item.append(y)\n",
    "                    \n",
    "    df_main = pd.DataFrame({\"association\":perm_item,\"support_ov\":sup_ov,\"support_sing\":sup_sing})\n",
    "    df_main = df_main.assign(confidence = lambda x:round(((x.support_ov/x.support_sing)*100),0))\n",
    "    return df_main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Finding the max k value in the given set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    max(df_supp[\"k_val\"])\n",
    "except:\n",
    "    print(\"No max was found...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### This is where I iteratively call the confidence() function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_frames = []\n",
    "try:\n",
    "    if len(df_supp[\"k_val\"]) != 0 : \n",
    "        for lp in range(1,max(df_supp[\"k_val\"])+1):\n",
    "            #print(lp)\n",
    "            #\n",
    "            df_0 = confidence(lp)\n",
    "            df_0 = df_0[df_0.support_sing != 0]\n",
    "            df_frames.append(df_0)\n",
    "        df_associations = pd.concat(df_frames)\n",
    "        display(df_associations.head())\n",
    "except:\n",
    "   print(\"No items or transactions meet the user requirements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Concat the Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_associations = pd.concat(df_frames)\n",
    "    display(df_associations)\n",
    "except:\n",
    "    print(\"No items or transactions meet the user requirements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Making sure that user parameters are met for confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_associations = df_associations[df_associations['confidence'] >= confidence_percent]\n",
    "    display(df_associations)\n",
    "except:\n",
    "    print(\"No items or transactions meet the user requirements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Formatting the Dataframe Final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df_final = df_associations.reset_index().drop(['index','support_sing'],axis=1)\n",
    "    df_final.columns = [\"Association\",\"Support\",\"Confidence\"]\n",
    "except:\n",
    "    print(\"No items or transactions meet the user requirements!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Associations "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    print(\"Store Name: \"+ str(ns(int(store_num))))\n",
    "    print(\"\\nFinal Associations that meet the user standards....\")\n",
    "    print(\"Support: \" + str(support_percent) + \"%\" + \"\\t\" + \"Confidence: \" + str(confidence_percent) + '%')\n",
    "    #this will display the max column width so we can see the associations involved....\n",
    "    pd.set_option('display.max_colwidth', 0)\n",
    "    display(df_final)\n",
    "except:\n",
    "    print(\"\\nNo Associations were generated based on the parameters set!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
